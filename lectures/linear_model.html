<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Arthur Allignol" />
  <title>Linear Model</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="/data/git/reveal.js/css/reveal.css"/>
  <style type="text/css">code{white-space: pre;}</style>

<!-- For syntax highlighting using highlight.js-->
    <link rel="stylesheet" href="/data/git/reveal.js/lib/css/github.css">
  
  <link rel="stylesheet" href="/data/git/reveal.js/css/theme/white.css" id="theme">
  <!-- If the query includes 'print-pdf', include the PDF print sheet -->
  <script>
    if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = '/data/git/reveal.js/css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
  </script>
  <!--[if lt IE 9]>
  <script src="/data/git/reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Linear Model</h1>
    <h2 class="author">Arthur Allignol</h2>
    <h3 class="date"></h3>
</section>

<section><section id="introduction" class="titleslide slide level1"><h1>Introduction</h1></section><section id="introduction-1" class="slide level2">
<h1>Introduction</h1>
<p>Linear modelling is used for explaining the relationship between a single quantitive variable <span class="math inline">\(Y\)</span> and one or more explanatory variables <span class="math inline">\(X_1, \dots, X_q\)</span></p>
<ul>
<li><span class="math inline">\(Y\)</span> is also called the <em>response, outcome, dependent variable</em></li>
<li>The <span class="math inline">\(X\)</span>’s are the <em>predictors, independent, explanatory</em> variables
<ul>
<li>They can be of any type (continuous, ordinal, …)</li>
</ul></li>
</ul>
<p>A very general model would be</p>
<p><span class="math display">\[ 
Y = f(X_1, \dots, X_q) + \varepsilon, 
\]</span></p>
<p>with <span class="math inline">\(f\)</span> some unknown function and <span class="math inline">\(\varepsilon\)</span> the error. Assume a more restricted form for the model:</p>
<p><span class="math display">\[ 
Y = \beta_0 + \beta_1X1 + \dots + \beta_qX_q + \varepsilon,
\]</span></p>
<p>which is a linear model</p>
</section><section id="introduction-2" class="slide level2">
<h1>Introduction</h1>
<p>Assume <span class="math inline">\(y_i\)</span> represents the value of the response variable on the <span class="math inline">\(i\)</span>th individual, and that <span class="math inline">\(x_{i1}, x_{i2}, \dots, x_{iq}\)</span> represents the individual’s values on <span class="math inline">\(p\)</span> explanatory variables, with <span class="math inline">\(i = 1, \dots, n\)</span>.</p>
<p>The multiple linear regression model is given by <span class="math display">\[
y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_q x_{iq} + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span></p>
</section><section id="introduction-3" class="slide level2">
<h1>Introduction</h1>
<p>Thus, the distribution of <span class="math inline">\(Y\)</span> is also normal with expected value given by the linear combination of the explanatory variables</p>
<p><span class="math display">\[
E(y | x_1, \dots, x_q) = \beta_0 + \beta_1 x_{1} + \dots + \beta_q x_{q}
\]</span></p>
<p>and with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The parameters of the model <span class="math inline">\(\beta_k\)</span>, <span class="math inline">\(k = 1, \dots, q\)</span>, are known as regression coefficients with <span class="math inline">\(\beta_0\)</span> the <em>intercept</em></p>
</section><section id="introduction-4" class="slide level2">
<h1>Introduction</h1>
<p>It will be convenient to collect the <span class="math inline">\(n\)</span> responses <span class="math inline">\(\mathbf{y}\)</span>, viewed as a realisation of a random vector <span class="math inline">\(\mathbf{Y}\)</span> with mean</p>
<p><span class="math display">\[
\text{E}(\mathbf{Y}) = \boldsymbol\mu
\]</span></p>
<p>and variance-covariance matrix</p>
<p><span class="math display">\[
\text{var}(\mathbf{Y}) = \sigma^2\mathbf{I}
\]</span></p>
<p>Under the assumption of normality, <span class="math inline">\(\mathbf{Y}\)</span> has a multivariate normal distribution</p>
<p><span class="math display">\[
\mathbf{Y} \sim {\cal N}(\boldsymbol\mu, \sigma^2\mathbf{I})
\]</span></p>
</section><section id="introduction-5" class="slide level2">
<h1>Introduction</h1>
<p>The model can be rewritten in matrix form as</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{X} \boldsymbol\beta + \varepsilon,
\]</span></p>
<p>or equivalently</p>
<p><span class="math display">\[
\boldsymbol\mu = \mathbf{X} \boldsymbol\beta,
\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> a <span class="math inline">\(n \times p\)</span> matrix containing the values of the <span class="math inline">\(q\)</span> predictors and the intercept</li>
<li><span class="math inline">\(\mathbf{X}\)</span> usually called <em>model matrix</em> or <em>design matrix</em></li>
<li><span class="math inline">\(\mathbf{X} \boldsymbol\beta\)</span> is called <em>linear predictor</em></li>
</ul>
</section></section>
<section><section id="estimation" class="titleslide slide level1"><h1>Estimation</h1></section><section id="estimation-of-beta" class="slide level2">
<h1>Estimation of <span class="math inline">\(\beta\)</span></h1>
<p>Considering i.i.d observations and <span class="math inline">\(Y\)</span> normally distributed, the log-likelihood is</p>
<p><span class="math display">\[
\log L(\beta, \sigma^2) = -\frac{n}{2}\log(2\pi\sigma^2) -
\frac{1}{2}\sum (y_i - x_i&#39;\beta)^2/\sigma^2
\]</span></p>
<p>Maximising the log-likelihood wrt <span class="math inline">\(\beta\)</span> for a fixed value of <span class="math inline">\(\sigma^2\)</span> is equivalent to minimising the sum of squared differences between observed and expected values, or <em>residual sum of squares</em></p>
<p><span class="math display">\[ 
\text{RSS}(\beta) = \sum (y_i - x_i&#39;\beta)^2 = 
({\bf y} - {\bf X}\boldsymbol\beta)&#39;({\bf y} - {\bf
X}\boldsymbol\beta) 
\]</span></p>
</section><section id="estimation-of-beta-1" class="slide level2">
<h1>Estimation of <span class="math inline">\(\beta\)</span></h1>
<p>Taking derivatives of the residual sum of squares with respect to <span class="math inline">\(\beta\)</span> and setting the derivative equal to zero leads to the so-called <em>normal equations</em> for the maximum-likelihood estimator <span class="math inline">\(\hat{\beta}\)</span></p>
<p><span class="math display">\[ 
{\bf X}&#39;{\bf X}\hat{\boldsymbol\beta} = {\bf X}{\bf y}
\]</span></p>
<p>If the model matrix <span class="math inline">\({\mathbf{X}}\)</span> has full rank, i.e., <span class="math inline">\({\mathbf{X}}&#39;{\mathbf{X}}\)</span> is non-singular, then the <em>ordinary least squares</em> (OLS) or maximum likelihood estimator of the linear parameters is</p>
<p><span class="math display">\[
{\hat{\beta}}= ({\mathbf{X}}&#39;{\mathbf{X}})^{-1}{\mathbf{X}}&#39;{\mathbf{y}}\]</span></p>
</section><section id="properties-of-the-estimator-beta" class="slide level2">
<h1>Properties of the estimator <span class="math inline">\(\beta\)</span></h1>
<p>If the model is correct</p>
<p><span class="math display">\[{\text{E}}({\hat{\beta}}) = \beta.
\]</span></p>
<p>Still assuming i.i.d observations and constant variance <span class="math inline">\(\sigma^2\)</span>, the variance-covariance matrix of the OLS estimator is</p>
<p><span class="math display">\[
{\text{var}}{{\hat{\beta}}} = ({\mathbf{X}}&#39;{\mathbf{X}})^{-1}\sigma^2
\]</span></p>
<p>In large samples, <span class="math inline">\({\hat{\beta}}\sim {\cal N}(\beta, ({\mathbf{X}}&#39;{\mathbf{X}})^{-1}\sigma^2)\)</span></p>
</section><section id="estimation-of-sigma2" class="slide level2">
<h1>Estimation of <span class="math inline">\(\sigma^2\)</span></h1>
<p>Plugging-in <span class="math inline">\({\hat{\beta}}\)</span> in the log-likelihood, we obtain</p>
<p><span class="math display">\[
\log \text{L}(\sigma^2) = -\frac{n}{2}\log(2\pi\sigma^2) -
\frac{1}{2}\sum (y_i - x_i&#39;{\hat{\beta}})^2/\sigma^2
\]</span></p>
<p>The maximum likelihood estimator is</p>
<p><span class="math display">\[
\hat\sigma^2 = \sum (y_i - x_i&#39;{\hat{\beta}})^2 / n
\]</span></p>
<p>Dividing by <span class="math inline">\(n - p\)</span> instead of <span class="math inline">\(n\)</span> leads to an unbiased estimator</p>
</section><section id="illustration-hubble-data-set" class="slide level2">
<h1>Illustration: Hubble data set</h1>
<p>From Freedman et al. (2001)</p>
<ul>
<li>Relative velocity and the distance of 24 galaxies according to measurements made by the Hubble telescope</li>
<li><code>velocity</code> assessed by measuring the Doppler red shift (km/s)</li>
<li><code>distance</code> (Mega parsecs <span class="math inline">\(=3.09 \times 10^{19}\)</span> km)</li>
</ul>
<p>Aim is to derive the age of the universe</p>
</section></section>
<section><section id="hypothesis-testing" class="titleslide slide level1"><h1>Hypothesis testing</h1></section><section id="wald-test" class="slide level2">
<h1>Wald test</h1>
<p>Test on one particular coefficient, e.g.,</p>
<p><span class="math display">\[ 
H_0: \beta_j = 0
\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\({\hat{\beta}}_j\)</span> has a distribution with mean 0 and variance given by the <span class="math inline">\(j\)</span>-th diagonal element of <span class="math inline">\({\text{var}}{{\hat{\beta}}}\)</span>. The test is based on the ratio</p>
<p><span class="math display">\[
t = \frac{{\hat{\beta}}_j}{\sqrt{{\text{var}}{\hat{\beta}}_j}}
\]</span></p>
<p>which follows a <em>Student</em> distribution with <span class="math inline">\(n - p\)</span> degrees of freedom under the null hypothesis when <span class="math inline">\(\sigma^2\)</span> is estimated (which is the case in practice)</p>
<p>Confidence intervals for the coefficients can be constructed based on the Wald test, i.e., we can state with <span class="math inline">\(100(1 - \alpha)\)</span> confidence that <span class="math inline">\({\hat{\beta}}_j\)</span> is between the bounds</p>
<p><span class="math display">\[
{\hat{\beta}}_j \pm t_{1 - \alpha/2; n - p}\sqrt{{\text{var}}{\hat{\beta}}_j}
\]</span></p>
</section><section id="likelihood-ratio-test" class="slide level2">
<h1>Likelihood ratio test</h1>
<p>Consider testing the joint significance of several coefficients, e.g.,</p>
<p><span class="math display">\[ 
H_0: \beta_2 = 0,
\]</span></p>
<p>where <span class="math inline">\(\beta = (\beta_1, \beta_2)\)</span>. Anagously, we partition the design matrix into <span class="math inline">\({\mathbf{X}}= ({\mathbf{X}}_1, {\mathbf{X}}_2)\)</span>. The hypothesis of interest states that the response does not depend on the last <span class="math inline">\(p_2\)</span> predictors.</p>
<p>The idea of the likelihood ratio test is</p>
<ol type="1">
<li>fit two nested models: a smaller model with the first <span class="math inline">\(p_1\)</span> predictors in <span class="math inline">\({\mathbf{X}}_1\)</span>, and a larger model with all <span class="math inline">\(p\)</span> predictors in <span class="math inline">\({\mathbf{X}}\)</span></li>
<li>compare their maximized likelihoods (or log-likelihoods)</li>
</ol>
</section><section id="likelihood-ratio-test-1" class="slide level2">
<h1>Likelihood ratio test</h1>
<p>We fit the smaller model with <span class="math inline">\({\mathbf{X}}_1\)</span> as predictors. The maximised log-likelihood is</p>
<p><span class="math display">\[ 
\max\log L(\beta_1) = c - \frac{1}{2}\text{RSS}({\mathbf{X}}_1)/\sigma^2,
\]</span></p>
<p>with <span class="math inline">\(c = -(n/2)\log(2\pi\sigma^2)\)</span></p>
<p>The maximised likelihood for the model including all predictors <span class="math inline">\({\mathbf{X}}_1 + {\mathbf{X}}_2\)</span> is</p>
<p><span class="math display">\[ 
\max\log L(\beta_1, \beta_2) = c - \frac{1}{2}\text{RSS}({\mathbf{X}}_1 + {\mathbf{X}}_2)/\sigma^2,
\]</span></p>
</section><section id="likelihood-ratio-test-2" class="slide level2">
<h1>Likelihood ratio test</h1>
<p>The likelihood ratio criterion is</p>
<p><span class="math display">\[
-2\log\Lambda = \frac{\text{RSS}({\mathbf{X}}_1) - \text{RSS}(X_1 +
X_2)}{\sigma^2}
\]</span></p>
<p>(where <span class="math inline">\(\Lambda\)</span> is the actual ratio of likelihoods)</p>
<ul>
<li>The idea is to look at the reduction in the residual sum of squares when adding the predictors in <span class="math inline">\({\mathbf{X}}_2\)</span> in the model</li>
<li>This reduction is compared to the error variance <span class="math inline">\(\sigma^2\)</span></li>
</ul>
<p>In practice we calculate the criterion using <span class="math inline">\(\hat{\sigma}^2 = \text{RSS}({\mathbf{X}}_1 + X_2)/(n - p)\)</span></p>
<ul>
<li><span class="math inline">\(-2\log\Lambda \sim \chi^2_{p_2}\)</span></li>
</ul>
</section><section id="f-test" class="slide level2">
<h1>F-test</h1>
<p>Under the assumption of normality, <span class="math inline">\(-2\log\Lambda\)</span> has an <em>exact</em> chi-squared distribution with <span class="math inline">\(p_2\)</span> d.f if <span class="math inline">\(\sigma^2\)</span> is known</p>
<p>If <span class="math inline">\(\sigma^2\)</span> is estimated, the criterion divided by <span class="math inline">\(p_2\)</span>, i.e.,</p>
<p><span class="math display">\[
F = \frac{(\text{RSS}({\mathbf{X}}_1) - \text{RSS}({\mathbf{X}}_1 +
{\mathbf{X}}_2))/p_2}{\text{RSS}({\mathbf{X}}_1 + {\mathbf{X}}_2)/(n - p)}
\]</span></p>
<p>has an exact <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(p_2\)</span> and <span class="math inline">\(n - p\)</span> d.f</p>
</section><section id="anova-table" class="slide level2">
<h1>Anova table</h1>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 51%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="header">
<th>Source of variation</th>
<th>Sum of squares</th>
<th>Degrees of freedoms</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\({\mathbf{X}}_1\)</span></td>
<td><span class="math inline">\(\text{RSS}(\emptyset) - \text{RSS}({\mathbf{X}}_1)\)</span></td>
<td><span class="math inline">\(p_1 - 1\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\({\mathbf{X}}_2\)</span> given <span class="math inline">\({\mathbf{X}}_1\)</span></td>
<td><span class="math inline">\(\text{RSS}({\mathbf{X}}_1) - \text{RSS}({\mathbf{X}}_1 + {\mathbf{X}}_2)\)</span></td>
<td><span class="math inline">\(p_2\)</span></td>
</tr>
<tr class="odd">
<td>Residual</td>
<td><span class="math inline">\(\text{RSS}({\mathbf{X}}_1 + {\mathbf{X}}_2)\)</span></td>
<td><span class="math inline">\(n - p\)</span></td>
</tr>
<tr class="even">
<td>Total</td>
<td><span class="math inline">\(\text{RSS}(\emptyset)\)</span></td>
<td><span class="math inline">\(n - 1\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>More detailed analysis of variance tables may be obtained by introducing the predictors one at a time, while keeping track of the reduction in residual sum of squares at each step</li>
</ul>
</section></section>
<section><section id="contrasts" class="titleslide slide level1"><h1>Contrasts</h1></section><section id="contrasts-1" class="slide level2">
<h1>Contrasts</h1>
<ul>
<li>Linear regression can be extended to accomodate categorical variables (factors) using <em>dummy variables</em> or <em>contrasts</em></li>
<li>Below a categorical variable is represented by a dummy regressor <span class="math inline">\(D\)</span>, coded <span class="math inline">\(1\)</span> for one category, <span class="math inline">\(0\)</span> for the other</li>
</ul>
<p><span class="math display">\[
y_i = \beta_0 + \beta X_i + \gamma D_i + \varepsilon_i
\]</span></p>
<ul>
<li>This fits two regression lines with the same slope but different intercepts, i.e., <span class="math inline">\(\gamma\)</span> represents the constant separation between 2 regression lines</li>
</ul>
<p><span class="math display">\[ 
\begin{align}
y_i &amp;= \beta_0 + \beta X_i + \gamma(0) + \varepsilon_i = \beta_0 +
\beta X_i + \varepsilon_i \\
y_i &amp;= \beta_0 + \beta X_i + \gamma(1) + \varepsilon_i = \beta_0 +
\gamma + \beta X_i + \varepsilon_i \\
\end{align}
\]</span></p>
</section><section id="contrasts-for-categorical-variables" class="slide level2">
<h1>Contrasts for categorical variables</h1>
<ul>
<li>A variable with <span class="math inline">\(m\)</span> categories has <span class="math inline">\(m - 1\)</span> regressors</li>
<li>As with the two category case, one of the categories is a reference group (coded 0 for all dummy regressors), e.g.,</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Category</th>
<th><span class="math inline">\(D_1\)</span></th>
<th><span class="math inline">\(D_2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>category 1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>category 2</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>category 3</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The regression model is then</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta X_i + \gamma_1 D_{i1} + \gamma_2 D_{i2} + \varepsilon_i
\]</span></p>
</section><section id="contrasts-for-categorical-variables-1" class="slide level2">
<h1>Contrasts for categorical variables</h1>
<p>This model describes 3 parallel regression lines, which can differ in their intercepts</p>
<ul>
<li>Category 3: <span class="math inline">\(y_i = (\beta_0 + \gamma_2) + \beta X_i + \varepsilon_i\)</span></li>
<li>Category 2: <span class="math inline">\(y_i = (\beta_0 + \gamma_1) + \beta X_i + \varepsilon_i\)</span></li>
<li>Category 1: <span class="math inline">\(y_i = \beta_0 + \beta X_i + \varepsilon_i\)</span></li>
</ul>
<p>“category 1” is coded as 0 for both dummy regressors, thus serves as <em>baseline</em> category with which the other categories are compared</p>
<p>These types of contrasts are called <em>treatment contrasts</em></p>
</section><section id="choice-of-baseline-category" class="slide level2">
<h1>Choice of baseline category</h1>
<p>The choice of reference category is technically irrelevant, but there are two possible consideration</p>
<ul>
<li>Theory may suggest we compare to a particular category, e.g., non smoker VS 10 cigarettes a day, and non-smokers VS a pack/day</li>
<li>The largest category gives the smallest standard errors</li>
</ul>
<p>In R, factors take care of the dummy coding in the design matrix. By default the reference level is the lowest level in alphabetical order.</p>
<p>In SAS, the <code>class</code> statement is used. Reference level is the last one.</p>
<p>N.B.: other types of contrasts exist. In particular, constrats can be defined in such a way as to test specific hypotheses</p>
</section></section>
<section><section id="regression-diagnostics" class="titleslide slide level1"><h1>Regression diagnostics</h1></section><section id="regression-diagnostics-1" class="slide level2">
<h1>Regression Diagnostics</h1>
<p>The estimation of and inference from the regression model depend on several assumptions. These should be checked using <em>regression diagnostics</em>.</p>
<p>The potential problems:</p>
<ul>
<li><strong>Error:</strong> We assume <span class="math inline">\(\varepsilon \sim {\cal N}(0, \sigma^2{\mathbf{I}})\)</span></li>
<li><strong>Model:</strong> We assume that the structural part of the model, <span class="math inline">\({\text{E}}y =  X\beta\)</span>, is correct</li>
<li><strong>Unusual observations:</strong> Sometimes just a few observations do not fit the model. These few observations might change the choice and fit of the model</li>
</ul>
</section><section id="diagnostic-plots" class="slide level2">
<h1>Diagnostic plots</h1>
<ul>
<li>A plot of residuals against each explanatory variable in the model. The presence of a non-linear relationship, for example, may suggest that a higher-order term in the explanatory variable should be considered.</li>
<li>A plot of residuals against fitted values. If the variance of the residuals appears to increase with predicted value, a transformation of the response variable may be in order.</li>
<li>A normal probability plot of the residuals. After all the systematic variation has been removed from the data, the residuals should look like a sample from a standard normal distribution. A plot of the ordered residuals against the expected order statistics from a normal distribution provides a graphical check of this assumption.</li>
</ul>
</section><section id="cooks-distance" class="slide level2">
<h1>Cook’s distance</h1>
<p>A further diagnostic that is often very useful is an index plot of the Cook’s distances for each observation. This statistic is defined as</p>
<p><span class="math display">\[ 
D_k = \frac{1}{(p + 1)\hat{\sigma}^2} \sum_{i=1}^n (\hat{y}_{i(k)} - y_i)^2
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_{i(k)}\)</span> is the fitted value of the <span class="math inline">\(i\)</span>th observation when the <span class="math inline">\(k\)</span>th observation is omitted from the model.</p>
<p>The values of <span class="math inline">\(D_k\)</span> assess the impact of the <span class="math inline">\(k\)</span>th observation on the estimated regression coefficients. Values of <span class="math inline">\(D_k\)</span> greater than one are suggestive that the corresponding observation has undue influence on the estimated regression coefficients.</p>
</section><section id="partial-residual-plot" class="slide level2">
<h1>Partial residual plot</h1>
<p>Construct the response with the predicted effect of the other <span class="math inline">\(X\)</span> removed</p>
<p><span class="math display">\[
y - \sum_{j \neq i}x_j{\hat{\beta}}_j = \hat y + \hat\varepsilon - \sum_{j
\neq i}x_j{\hat{\beta}}_j = x_i{\hat{\beta}}_i + \hat\varepsilon
\]</span></p>
<p>The partial residual plot is then <span class="math inline">\(x_i{\hat{\beta}}_i + \hat\varepsilon\)</span> against <span class="math inline">\(x_i\)</span>. Additionally the <code>termplot</code> function in R centers the <span class="math inline">\(x_i\)</span>.</p>
<ul>
<li>Show the relationship between a given independent variable and the response variable given that other independent variables are also in the model.</li>
</ul>
</section><section id="illustrations" class="slide level2">
<h1>Illustrations</h1>
<p><strong>Hubble data set</strong></p>
<p><strong>A complete example: Insurance redlining</strong></p>
<p>Insurance redlining refers to the practice of refusing to issue insurances to certain types of people or within some geographical area.</p>
<p>In the 70s, several Chicago community organisations accused insurances of redlining their neighbourhood</p>
<p>To assess whether redlining was taking place, the number of FAIR (Fair Access to Insurance Requirements) plan policies by zip code were collected as well as some statistics at the zip code level</p>
<ul>
<li><code>race</code> racial composition in percent minority</li>
<li><code>fire</code> fires per 100 housing units</li>
<li><code>theft</code> theft per 1000 population</li>
<li><code>age</code> percent of housing units built before 1939</li>
<li><code>involact</code> new FAIR plan policies and renewals per 100 housing units</li>
<li><code>income</code> median family income in thousands of dollars</li>
<li><code>side</code> North or South side of Chicago</li>
</ul>
</section></section>
<section><section id="extensions" class="titleslide slide level1"><h1>Extensions</h1></section><section id="interactions" class="slide level2">
<h1>Interactions</h1>
<p>Two explanatory variables interact in determining a response variable when the partial effect of one depends on the value of the other</p>
<ul>
<li>If the regressions in different categories of a qualitative explanatory variable are not parallel, then the qualitative variable interacts with one or more of the quantitative variables</li>
<li>The dummy-regression model can be modified to reflect interactions</li>
</ul>
<p>The following model accomodates different intercepts and slopes for different categories (here 2)</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta X_i + \gamma D_i + \delta (D_iX_i) +
\varepsilon_i
\]</span></p>
</section><section id="interactions-1" class="slide level2">
<h1>Interactions</h1>
<ul>
<li><p>The interaction regressor is the product of the other two regressors.</p></li>
<li><p>For category 1 (coded 0)</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
y_i &amp;= \beta_0 + \beta X_i + \gamma(0) + \delta (0\cdot X_i) +
\varepsilon_i\\
 &amp;= \beta_0 + \beta X_i + \varepsilon_i
\end{align}
 \]</span></p>
<ul>
<li>For category 2 (coded 1)</li>
</ul>
<p><span class="math display">\[
\begin{align}
y_i &amp;= \beta_0 + \beta X_i + \gamma(1) + \delta (1\cdot X_i) +
\varepsilon_i\\
 &amp;= (\beta_0 + \gamma) + (\beta + \delta)X_i + \varepsilon_i
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta\)</span> are the intercept and slope for the regression of <span class="math inline">\(X\)</span> among category 1</li>
<li><span class="math inline">\(\gamma\)</span> gives the difference in intercept between the categories</li>
<li><span class="math inline">\(\delta\)</span> gives the difference in slopes between the groups</li>
</ul>
</section><section id="interactions-2" class="slide level2">
<h1>Interactions</h1>
<p><strong>Interactions between 2 factors:</strong></p>
<p>The model with interactions corresponds to having different levels of the two factors.</p>
<p><strong>Interactions between continuous variables:</strong></p>
<p>Tricky. The interpretation is that you have a linear effect of varying one variable while keeping the other constant, but with a slope that changes as you vary the other variable.</p>
<p><strong>N.B.</strong> If you include interactions in the model, it is better to also include the main effects, even if the effects are small.</p>
</section><section id="illustration" class="slide level2">
<h1>Illustration</h1>
<p><strong>Anorexia data set</strong></p>
<p>Weight change data for young female anorexia patients following treatment.</p>
<ul>
<li><code>Treat</code> with levels <code>Cont</code>: Control, <code>CBT</code>: Cognitive Behavioural treatment and <code>FT</code>: family treatment</li>
<li><code>Prewt</code> Weight of patient before study period, in lbs</li>
<li><code>Postwt</code> Weight of patient after study period, in lbs</li>
</ul>
</section><section id="transformations-of-the-response-andor-predictors" class="slide level2">
<h1>Transformations of the response and/or predictors</h1>
<p>Transformation of the response and/or predictors can improve the fit of the model and correct violations of model assumptions such as non-constant error variance.</p>
<p>We may also consider adding additional predictors that are functions of the existing predictors, quadratic terms</p>
</section><section id="transformation-of-the-response" class="slide level2">
<h1>Transformation of the response</h1>
<p>Suppose we want to use a log transformation of the response</p>
<p><span class="math display">\[ 
\log y = \beta_0 + \beta_1 X + \varepsilon.
\]</span></p>
<p>In the original scale of the response, the model becomes</p>
<p><span class="math display">\[ 
y = \exp(\beta_0 + \beta_1 X)\exp(\varepsilon)
\]</span></p>
<p>The error enters the model <em>multiplicatively</em>. Whether that’s sensible is a trial and error thing (redo the diagnostic plots and see)</p>
</section><section id="transformation-of-the-response-1" class="slide level2">
<h1>Transformation of the response</h1>
<ul>
<li>Regression coefficients are interpreted in the transformed scale (not always easy)</li>
<li>Regression coefficients for models where the transformations are different can’t be compared</li>
</ul>
<p>Use with care!</p>
<p>But note that a log-transformation allows for a meaningful interpretation of the regression coefficients</p>
<p><span class="math display">\[
\begin{align}
\log \hat{y} &amp;= {\hat{\beta}}_0 + {\hat{\beta}}_1X_1 + \dots + {\hat{\beta}}_pX_p\\
\hat{y} &amp;= e^{{\hat{\beta}}_0}e^{{\hat{\beta}}_1X_1} \dots e^{{\hat{\beta}}_pX_p}
\end{align}
\]</span></p>
<p>An increase of one <span class="math inline">\(X_1\)</span> would multiply the predicted response by <span class="math inline">\(e^{{\hat{\beta}}_1}\)</span></p>
</section><section id="transformation-of-the-predictors-polynomials" class="slide level2">
<h1>Transformation of the predictors — Polynomials</h1>
<p>Includes second order and higher powers of a variable in the model along with the original linear term. That is</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1x + \beta_2x^2 + \dots + \beta_kx^k + \varepsilon
\]</span></p>
<ul>
<li>Non linear relation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span></li>
<li>but the model is still linear</li>
</ul>
</section><section id="splines" class="slide level2">
<h1>Splines</h1>
<p>We consider cubic splines</p>
<ul>
<li>Divide the real line by an ordered set of points <span class="math inline">\(\{z_i\}\)</span> known as <em>knots</em></li>
<li>On the interval <span class="math inline">\([z_i, z_{i+1}]\)</span> the spline is a cubic polynomial</li>
<li>Impose continuity, and continuous first and second derivatives to ensure smoothness</li>
</ul>
</section><section id="correct-for-heteroskedasticity" class="slide level2">
<h1>Correct for heteroskedasticity</h1>
<ul>
<li>Heteroskedasticity does not bias the coefficient estimates; though not efficient</li>
<li>but the standard errors are incorrect, so are the confidence intervals and p-values</li>
</ul>
<p>We need robust estimates of the variance of the regression coefficients.</p>
<p>Won’t be detailed here but are available in, e.g., R-package <strong>sandwich</strong></p>
</section><section id="illustrations-1" class="slide level2">
<h1>Illustrations</h1>
<p>The simulated data</p>
</section></section>
    </div>
  </div>


  <script src="/data/git/reveal.js/lib/js/head.min.js"></script>
  <script src="/data/git/reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,         // Display controls in the bottom right corner
        progress: true,         // Display a presentation progress bar
        history: true,          // Push each slide change to the browser history
        center: true,                       // Vertical centering of slides
        maxScale: 1.5,                  // Bounds for smallest/largest possible content scale
        slideNumber: true,                // Display the page number of the current slide
        theme: 'white', // available themes are in /css/theme
        transition: 'convex', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: '/data/git/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: '/data/git/reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      { src: '/data/git/reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
            { src: '/data/git/reveal.js/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
            //          { src: '/data/git/reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: '/data/git/reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
    </body>
</html>
