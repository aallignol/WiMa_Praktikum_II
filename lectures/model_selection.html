<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Arthur Allignol" />
  <title>Model and Variable Selection</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="/data/git/reveal.js/css/reveal.css"/>
  <style type="text/css">code{white-space: pre;}</style>

<!-- For syntax highlighting using highlight.js-->
    <link rel="stylesheet" href="/data/git/reveal.js/lib/css/github.css">
  
  <link rel="stylesheet" href="/data/git/reveal.js/css/theme/white.css" id="theme">
  <!-- If the query includes 'print-pdf', include the PDF print sheet -->
  <script>
    if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = '/data/git/reveal.js/css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
  </script>
  <!--[if lt IE 9]>
  <script src="/data/git/reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Model and Variable Selection</h1>
    <h2 class="author">Arthur Allignol</h2>
    <h3 class="date"></h3>
</section>

<section><section id="introduction" class="titleslide slide level1"><h1>Introduction</h1></section><section id="why-do-we-want-to-choose-a-model" class="slide level2">
<h1>Why do we want to choose a model?</h1>
<p><strong>Explanation:</strong></p>
<p>Occam’s razor applies and we want</p>
<blockquote>
<p>an explanation that is as simple as possible, but not simpler</p>
</blockquote>
<p><strong>Prediction:</strong></p>
<p>For prediction, we want a model that predicts well (!) All that matters is that it works, the model doesn’t necessarily need to be interpretable</p>
</section><section id="bias-variance-trade-off" class="slide level2">
<h1>Bias-Variance trade-off</h1>
<p>Let <span class="math inline">\(Y = f(X) + \varepsilon\)</span>, with <span class="math inline">\({\text{E}}(\varepsilon) = 0\)</span> and <span class="math inline">\({\text{var}}\varepsilon = \sigma^2_\varepsilon\)</span>. The error of a regression fit <span class="math inline">\(\hat{f}(X)\)</span> at an input point <span class="math inline">\(x_0\)</span> is (using squared error)</p>
<p><span class="math display">\[
\begin{align}
\text{Err}(x_0) &amp;= {\text{E}}[(Y - \hat{f}(x_0))^2 | X = x_0] \\
&amp;= \sigma^2_\varepsilon + [{\text{E}}\hat{f}(x_0) - f(x_0)]^2 +
{\text{E}}[\hat{f}(x_0) - {\text{E}}\hat{f}(x_0)]^2 \\
&amp;= \sigma^2_\varepsilon + \text{Bias}^2(\hat{f(x_0)}) +
{\text{var}}(\hat{f}(x_0))\\
&amp;= \text{Irreducible error} + \text{Bias}^2 + \text{Variance}
\end{align}
\]</span></p>
<p>Typically, the more complex we make the model <span class="math inline">\(\hat{f}\)</span>, the lower the (squared) error but the higher the variance</p>
</section><section id="model-selection-is-difficult" class="slide level2">
<h1>Model selection is difficult</h1>
<pre class="r"><code>set.seed(442)
n &lt;- 200
p &lt;- 100
y &lt;- rnorm(n)
x &lt;- matrix(rnorm(n * p), nrow = n)
df &lt;- data.frame(y, x)
mdl &lt;- lm(y ~ ., df)</code></pre>
</section><section id="model-selection-is-difficult-1" class="slide level2">
<h1>Model selection is difficult</h1>
<pre class="r"><code>stars &lt;- 1 + which(coefficients(summary(mdl))[-1, 4] &lt; 0.05)
mdl.2 &lt;- lm(y ~ ., df[, c(1, stars)])
summary(mdl.2)</code></pre>
<pre><code>
Call:
lm(formula = y ~ ., data = df[, c(1, stars)])

Residuals:
     Min       1Q   Median       3Q      Max 
-2.56045 -0.73379  0.06531  0.63945  2.83769 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  0.00228    0.07710   0.030   0.9764  
X4          -0.15414    0.07373  -2.090   0.0379 *
X14          0.09399    0.07275   1.292   0.1979  
X40         -0.09558    0.07266  -1.315   0.1899  
X63         -0.17910    0.08089  -2.214   0.0280 *
X65         -0.19684    0.07736  -2.544   0.0117 *
X84         -0.12703    0.07221  -1.759   0.0801 .
X99          0.05094    0.07675   0.664   0.5077  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.048 on 192 degrees of freedom
Multiple R-squared:  0.09739,   Adjusted R-squared:  0.06449 
F-statistic:  2.96 on 7 and 192 DF,  p-value: 0.00573</code></pre>
</section><section id="model-selection-is-difficult-2" class="slide level2">
<h1>Model selection is difficult</h1>
<p>The standard asymptotic formulas for, e.g., standard errors, assume that the variables included in the model are <em>prespecified</em></p>
<ul>
<li>The “phantom degrees of freedom” induced by having a subjective assessment of the data is not taken into account</li>
<li>That leads to, e.g., too narrow confidence intervals → too small p-values</li>
</ul>
<p>Usual practice is to sweep this problem under the rug</p>
<p>One possible solution is too prespecify all variables in advance (including non-linear terms, interactions, …)</p>
<ul>
<li>That is the only way to conserve the advertised coverage rates without further ado</li>
</ul>
</section><section id="otherwise" class="slide level2">
<h1>Otherwise…</h1>
<p>There is plenty of techniques for performing model selections</p>
<ul>
<li>Subsetting methods</li>
<li>Different criteria for comparing models</li>
<li>Use models that allow for automatic variable selections, e.g.,
<ul>
<li>Random forest</li>
<li>Shrinkage</li>
</ul></li>
</ul>
<p>Inference is still a problem, though. Model selection is an active area of research</p>
</section></section>
<section><section id="model-selection-criteria" class="titleslide slide level1"><h1>Model selection Criteria</h1></section><section id="some-model-selection-criteria" class="slide level2">
<h1>Some model selection criteria</h1>
<p><strong>Akaike Information Criterion</strong> (1973)</p>
<p><span class="math display">\[\text{AIC}(M) = -2\log L_M + 2p\]</span></p>
<p>where <span class="math inline">\(L_M\)</span> is the likelihood of model <span class="math inline">\(M\)</span> and <span class="math inline">\(p\)</span> is the number of parameters</p>
<p>Akaike’s rule is to pick the model that minimises the AIC.</p>
<p><strong>Schwarz’s criterion (BIC: Bayesian Information Criterion)</strong></p>
<p><span class="math display">\[\text{BIC}(M) = -2\log L_M + p\log n\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the sample size</p>
<p>The intuition behind these 2 criteria is that</p>
<p><span class="math display">\[\text{AIC} = \text{bias} + \text{variance}\]</span></p>
</section><section id="some-model-selection-criteria-1" class="slide level2">
<h1>Some model selection criteria</h1>
<p><strong>Mallow’s <span class="math inline">\(C_p\)</span> statistic</strong></p>
<p>For a linear model with <span class="math inline">\(p + 1\)</span> parameters</p>
<p><span class="math display">\[C_p = \frac{1}{n} \sum_i (Y_i - \hat{y}_i)^2 +
\frac{2}{\hat{\sigma}^2}n(p + 1)\]</span></p>
<p><strong>Leave-one-out cross-validation</strong></p>
<p><span class="math display">\[
\text{LOOCV} = \frac{1}{n} \sum_i \left(Y_i - \hat{y}^{(-i)}\right)
\]</span></p>
<p>where <span class="math inline">\(\hat{y}^{(-i)}\)</span> denote the estimated prediction removing data <span class="math inline">\(i\)</span>.</p>
</section><section id="summary" class="slide level2">
<h1>Summary</h1>
<p>There is a theorem which says (roughy)</p>
<blockquote>
<p>As <span class="math inline">\(n \rightarrow \infty\)</span>, the expected out of sample MSE of the model picked out by LOOCV is close to that of the best model considered.</p>
</blockquote>
<p>For <span class="math inline">\(n\)</span> large, LOOCV, AIC and Marlow’s <span class="math inline">\(C_p\)</span> will tend to pick the same model</p>
<p>Another theorem says (roughly)</p>
<blockquote>
<p>As <span class="math inline">\(n \rightarrow \infty\)</span>, if the true model is among those being considered, LOOCV, AIC and <span class="math inline">\(C_p\)</span> will tend to pick models that are <em>strickly larger</em> than the truth</p>
</blockquote>
<p>which is ok for prediction</p>
<p>BIC will tend towards choosing the true model. For finite samples, BIC often chooses models that are too simple</p>
</section></section>
<section><section id="model-selection-strategies" class="titleslide slide level1"><h1>Model selection strategies</h1></section><section id="stepwise-model-selection" class="slide level2">
<h1>Stepwise model selection</h1>
<p><strong>Forward</strong> stepwise model selection starts with a small model (possibly just containing an intercept)</p>
<p>Consider all possible one-variable expansions of the model</p>
<p>Add the variable that is best according to some criterion</p>
<ul>
<li>smallest p-value</li>
<li>lowest AIC, BIC</li>
<li>…</li>
</ul>
<p>The process is repeated until the criterion stops improving</p>
</section><section id="stepwise-model-selection-1" class="slide level2">
<h1>Stepwise model selection</h1>
<p><strong>Backward</strong> stepwise model selection works the other way around</p>
<ul>
<li>Start with the largest model you are willing to consider</li>
<li>Eliminate variables one at a time choose the best variable to remove according to some criterion</li>
<li>Keep eliminating variables until we no longer improve</li>
</ul>
<p><strong>Mixed</strong> stepwise variable contemplates both adding and removing one variable at each step, and take the best step.</p>
<p>→ it is a greedy algorithm and like trees it is a high variance procedure</p>
<!-- ## $k$-fold Cross-validation ## -->
<!-- - Randomly divide the set of observations into $k$ groups or *folds* -->
<!--   of approximately equal size -->
<!-- - The first fold is treated as validation set while the tree is fitted -->
<!--   on the remaining $k - 1$ folds -->
<!-- - Compute the prediction error in the held-out fold  -->
<!-- $$L(Y^{-k}, \hat{f}^{-k}(X)) = (y^{-k} - \hat{f}^{-k}(x))^2$$ -->
<!-- - The procedure is repeated $k$-times resulting in $k$ estimates of -->
<!--   the test error -->
<!-- - The $k$-fold CV estimate is then -->
<!-- $$ -->
<!-- CV_{(k)} = \frac{1}{k}\sum_{j = 1}^k L(Y^{-j}, \hat{f}^{-j}(X)) -->
<!-- $$ -->
<!-- $k$ is typically set to 5 or 10. -->
<!-- - Repeat the whole procedure $B$ times for more stable estimates -->
</section><section id="shrinkage" class="slide level2">
<h1>Shrinkage</h1>
<p>The idea of <em>shrinkage</em> is to fit a model that contains all <span class="math inline">\(p\)</span> predictors</p>
<p>And use a technique that <em>constrains</em> or <em>regularizes</em> the coefficients estimates, or equivalently, that <em>shrinks</em> the coefficients toward 0</p>
<p>The aim is to reduce the variance</p>
<p>The two best known techniques are <em>ridge regression</em> and the <em>lasso</em></p>
</section><section id="ridge-regression" class="slide level2">
<h1>Ridge regression</h1>
<p>Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimise a penalised RSS (in the case of the linear model)</p>
<p><span class="math display">\[ \hat\beta^{\text{ridge}} = \operatorname*{arg\,min}_{\beta} \left\{\sum_{i = 1}^n
(y_i - {\mathbf{x}}\beta)^2 + \lambda\sum_{j = 1}^q \beta_j^2\right\} \]</span></p>
<p>where <span class="math inline">\(\lambda \geq 0\)</span> is a tuning parameter, to be determined separately.</p>
<p>The larger <span class="math inline">\(\lambda\)</span>, the greater the amount of shrinkage</p>
</section><section id="the-lasso" class="slide level2">
<h1>The lasso</h1>
<p>The lasso regression also shrinks the regression coefficients towards 0, but unlike ridge regression the penalty in the lasso has the effect of forcing some of the coefficients to be exactly equal to 0</p>
<p><span class="math display">\[ \hat\beta^{\text{lasso}} = \operatorname*{arg\,min}_{\beta} \left\{\sum_{i = 1}^n
(y_i - {\mathbf{x}}\beta)^2 + \lambda\sum_{j = 1}^q \lvert\beta_j\rvert\right\} \]</span></p>
<p>Thus the lasso is able to perform variable selection</p>
</section><section id="shrinkage-methods" class="slide level2">
<h1>Shrinkage methods</h1>
<p>One can show that the lasso and ridge regression solve the problems</p>
<p><span class="math display">\[\operatorname*{arg\,min}_{\beta} \left\{\sum_{i = 1}^n (y_i - {\mathbf{x}}\beta)^2 \right\}
\quad \text{subject to} \quad \sum_{j = 1}^q \lvert\beta_j\rvert \leq
s
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\operatorname*{arg\,min}_{\beta} \left\{\sum_{i = 1}^n (y_i - {\mathbf{x}}\beta)^2 \right\}
\quad \text{subject to} \quad \sum_{j = 1}^q \beta^2_j \leq s
\]</span></p>
<p>I.e., for every value of <span class="math inline">\(\lambda\)</span>, there is some <span class="math inline">\(s\)</span> such that the equations above will give the same lasso/ridge coefficient estimates</p>
</section><section id="shrinkage-methods-1" class="slide level2">
<h1>Shrinkage methods</h1>
<figure>
<img src="lasso_ridge.png" />
</figure>
</section><section id="shrinkage-methods-2" class="slide level2">
<h1>Shrinkage methods</h1>
<ul>
<li>The tuning parameter <span class="math inline">\(\lambda\)</span> can be selected via cross-validation</li>
</ul>
<p>Both methods result in biased regression coefficients estimates, but decreased variance</p>
<ul>
<li>The lasso performs variable selection as some coefficients are forced to be equal to 0</li>
<li>The lasso might perform better if one suspects that many coefficients are close to zero (as might be the case in bioinformatic problems)</li>
</ul>
<p>The predictors should be standardised, otherwise, possibly important variables with small coefficients (e.g., age) might be dropped.</p>
</section><section id="dimension-reduction" class="slide level2">
<h1>Dimension reduction</h1>
<p>The methods discussed so far have controlled variance in two ways</p>
<ul>
<li>by using a subset of the original variables</li>
<li>by shrinking the coefficients towards 0</li>
</ul>
<p>All these methods have used all the original predictors <span class="math inline">\(X_1, X_2, \dots, X_q\)</span>.</p>
<p>Another possibility is to <em>transform</em> the predictors and then fit a model using the transformed variables</p>
<ul>
<li><em>Principal components regression:</em> PCA is a technique for reducing the dimension of a <span class="math inline">\(n \times q\)</span> data matrix X</li>
<li>Partial least squares</li>
<li><em>Propensity score analysis:</em> <span class="math inline">\(=\)</span> Probability to receive the treatment given the covariates</li>
</ul>
</section></section>
<section><section id="inference-after-model-selection" class="titleslide slide level1"><h1>Inference after model selection</h1></section><section id="inference-after-model-selection-1" class="slide level2">
<h1>Inference after model selection</h1>
<p>It’s <strong>BROKEN</strong>!!!</p>
<p>The classical statistical theory assumes that the variables that enter the model are prespecified, i.e., the design matrix is fixed.</p>
<p>The model selection process implies a <em>random</em> design matrix. That is not taken into account by the classical theory</p>
<p>Consequences are (usually) too small standard errors → too narrow confidence intervals → too small p-values</p>
</section><section id="inference-after-model-selection-2" class="slide level2">
<h1>Inference after model selection</h1>
<p>What works is</p>
<ul>
<li>Don’t do variable selection</li>
<li>Make inference on a another data set, e.g,
<ol type="1">
<li>Divide the data set in two equal parts at random</li>
<li>Build the model in the first part using one of the technique presented before</li>
<li>Make inference in the second part</li>
</ol></li>
</ul>
<p>The obvious drawback is the loss of power for the model building part</p>
<ul>
<li>Some complicated (and very recent) resampling based methods</li>
<li>Model averaging</li>
</ul>
<p>Automatic techniques for variable prevent you to think!</p>
</section></section>
    </div>
  </div>


  <script src="/data/git/reveal.js/lib/js/head.min.js"></script>
  <script src="/data/git/reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,         // Display controls in the bottom right corner
        progress: true,         // Display a presentation progress bar
        history: true,          // Push each slide change to the browser history
        center: true,                       // Vertical centering of slides
        maxScale: 1.5,                  // Bounds for smallest/largest possible content scale
        slideNumber: true,                // Display the page number of the current slide
        theme: 'white', // available themes are in /css/theme
        transition: 'convex', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: '/data/git/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: '/data/git/reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      { src: '/data/git/reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
            { src: '/data/git/reveal.js/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
            //          { src: '/data/git/reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: '/data/git/reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
    </body>
</html>
